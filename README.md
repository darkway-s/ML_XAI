# Explanations for Resnet18 and Resnet50 using Grad-CAM and LIME methods

> Tree 2023.11.29



This project delves into the interpretability of Convolutional Neural Network (CNN) models through the examination of classification results for 10 test images. The primary objective is to unravel the decision-making processes of two pre-trained CNN models, Resnet18 and Resnet50. To achieve this, two interpretability methods, Grad-CAM and LIME, are employed.

Grad-CAM allows for the generation of heatmaps that highlight the regions of an image contributing most to the model's prediction. Simultaneously, the Local Interpretable Model-agnostic Explanations (LIME) method, provides insights into model predictions by perturbing input data and observing the resulting changes in predictions.

Through the juxtaposition of these interpretability methods, this project aims to offer a nuanced understanding of the decision boundaries and prediction behaviors of the chosen CNN models. The ensuing report will comprehensively present predictive scores, visualization through heatmaps, and discussions on the results, fostering a deeper insight into the interpretability of complex neural network models.



For details, see in [Report](report.md)